<!DOCTYPE html>
<html lang="en">
  <head>
    <title>SpeechToTextV1  Reference</title>
    <link rel="stylesheet" type="text/css" href="css/jazzy.css" />
    <link rel="stylesheet" type="text/css" href="css/highlight.css" />
    <meta charset='utf-8'>
    <script src="js/jquery.min.js" defer></script>
    <script src="js/jazzy.js" defer></script>
    
  </head>
  <body>
    <a title="SpeechToTextV1  Reference"></a>
    <header>
      <div class="content-wrapper">
        <p><a href="index.html">SpeechToTextV1 Docs</a></p>
        <p class="header-right"><a href="https://github.com/watson-developer-cloud/swift-sdk"><img src="img/gh.png"/>View on GitHub</a></p>
      </div>
    </header>
    <div class="content-wrapper">
      <p id="breadcrumbs">
        <a href="index.html">SpeechToTextV1 Reference</a>
        <img id="carat" src="img/carat.png" />
        SpeechToTextV1  Reference
      </p>
    </div>
    <div class="content-wrapper">
      <nav class="sidebar">
        <ul class="nav-groups">
          <li class="nav-group-name">
            <a href="Guides.html">Guides</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="readme.html">README</a>
              </li>
            </ul>
          </li>
          <li class="nav-group-name">
            <a href="Classes.html">Classes</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Classes/SpeechToText.html">SpeechToText</a>
              </li>
              <li class="nav-group-task">
                <a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a>
              </li>
            </ul>
          </li>
          <li class="nav-group-name">
            <a href="Structs.html">Structures</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Structs/AcousticModel.html">AcousticModel</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AcousticModel/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AcousticModels.html">AcousticModels</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioDetails.html">AudioDetails</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioDetails/TypeEnum.html">– TypeEnum</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioDetails/Compression.html">– Compression</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioListing.html">AudioListing</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioListing/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioResource.html">AudioResource</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioResource/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/AudioResources.html">AudioResources</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Corpora.html">Corpora</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Corpus.html">Corpus</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Corpus/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/CustomWord.html">CustomWord</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Grammar.html">Grammar</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Grammar/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Grammars.html">Grammars</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/KeywordResult.html">KeywordResult</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/LanguageModel.html">LanguageModel</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/LanguageModel/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/LanguageModels.html">LanguageModels</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/RecognitionJob.html">RecognitionJob</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/RecognitionJob/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/RecognitionJobs.html">RecognitionJobs</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/RecognitionSettings.html">RecognitionSettings</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/RegisterStatus.html">RegisterStatus</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/RegisterStatus/Status.html">– Status</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SpeakerLabelsResult.html">SpeakerLabelsResult</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SpeechModel.html">SpeechModel</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SpeechModels.html">SpeechModels</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SpeechRecognitionAlternative.html">SpeechRecognitionAlternative</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SpeechRecognitionResult.html">SpeechRecognitionResult</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SpeechRecognitionResults.html">SpeechRecognitionResults</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SpeechRecognitionResultsAccumulator.html">SpeechRecognitionResultsAccumulator</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/SupportedFeatures.html">SupportedFeatures</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Word.html">Word</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/WordAlternativeResult.html">WordAlternativeResult</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/WordAlternativeResults.html">WordAlternativeResults</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/WordConfidence.html">WordConfidence</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/WordError.html">WordError</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/WordTimestamp.html">WordTimestamp</a>
              </li>
              <li class="nav-group-task">
                <a href="Structs/Words.html">Words</a>
              </li>
            </ul>
          </li>
          <li class="nav-group-name">
            <a href="Typealiases.html">Type Aliases</a>
            <ul class="nav-group-tasks">
              <li class="nav-group-task">
                <a href="Typealiases.html#/WatsonError">WatsonError</a>
              </li>
              <li class="nav-group-task">
                <a href="Typealiases.html#/WatsonResponse">WatsonResponse</a>
              </li>
            </ul>
          </li>
        </ul>
      </nav>
      <article class="main-content">
        <section>
          <section class="section">
            
            <h1 id='speech-to-text' class='heading'>Speech to Text</h1>

<p>The IBM Watson Speech to Text service enables you to add speech transcription capabilities to your application. It uses machine intelligence to combine information about grammar and language structure to generate an accurate transcription. Transcriptions are supported for various audio formats and languages.</p>

<p>The <code><a href="Classes/SpeechToText.html">SpeechToText</a></code> class is the SDK&rsquo;s primary interface for performing speech recognition requests. It supports the transcription of audio files, audio data, and streaming microphone data. Advanced users, however, may instead wish to use the <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code> class that exposes more control over the WebSockets session.</p>

<p>Please be sure to include both <code>SpeechToTextV1.framework</code> and <code>Starscream.framework</code> in your application. Starscream is a recursive dependency that adds support for WebSockets sessions.</p>

<p>Beginning with iOS 10+, any application that accesses the microphone must include the <code>NSMicrophoneUsageDescription</code> key in the app&rsquo;s <code>Info.plist</code> file. Otherwise, the app will crash. Find more information about this <a href="https://forums.developer.apple.com/thread/61521">here</a>.</p>
<h3 id='recognition-request-settings' class='heading'>Recognition Request Settings</h3>

<p>The <code><a href="Structs/RecognitionSettings.html">RecognitionSettings</a></code> class is used to define the audio format and behavior of a recognition request. These settings are transmitted to the service when <a href="https://console.bluemix.net/docs/services/speech-to-text/websockets.html#WSstart">initating a request</a>.</p>

<p>The following example demonstrates how to define a recognition request that transcribes WAV audio data with interim results:</p>
<pre class="highlight swift"><code><span class="k">var</span> <span class="nv">settings</span> <span class="o">=</span> <span class="kt">RecognitionSettings</span><span class="p">(</span><span class="nv">contentType</span><span class="p">:</span> <span class="s">"audio/wav"</span><span class="p">)</span>
<span class="n">settings</span><span class="o">.</span><span class="n">interimResults</span> <span class="o">=</span> <span class="kc">true</span>
</code></pre>

<p>See the <a href="http://watson-developer-cloud.github.io/swift-sdk/services/SpeechToTextV1/Structs/RecognitionSettings.html">class documentation</a> or <a href="https://console.bluemix.net/docs/services/speech-to-text/index.html">service documentation</a> for more information about the available settings.</p>
<h3 id='microphone-audio-and-compression' class='heading'>Microphone Audio and Compression</h3>

<p>The Speech to Text framework makes it easy to perform speech recognition with microphone audio. The framework internally manages the microphone, starting and stopping it with various method calls (<code>recognizeMicrophone</code> and <code>stopRecognizeMicrophone</code>, or <code>startMicrophone</code> and <code>stopMicrophone</code>).</p>

<p>There are two different ways that your app can determine when to stop the microphone:</p>

<ul>
<li><p>User Interaction: Your app could rely on user input to stop the microphone. For example, you could use a button to start/stop transcribing, or you could require users to press-and-hold a button to start/stop transcribing.</p></li>
<li><p>Final Result: Each transcription result has a <code>final</code> property that is <code>true</code> when the audio stream is complete or a timeout has occurred. By watching for the <code>final</code> property, your app can stop the microphone after determining when the user has finished speaking.</p></li>
</ul>

<p>To reduce latency and bandwidth, the microphone audio is compressed to OggOpus format by default. To disable compression, set the <code>compress</code> parameter to <code>false</code>.</p>

<p>It&rsquo;s important to specify the correct audio format for recognition requests that use the microphone:</p>
<pre class="highlight swift"><code><span class="c1">// compressed microphone audio uses the opus format</span>
<span class="k">let</span> <span class="nv">settings</span> <span class="o">=</span> <span class="kt">RecognitionSettings</span><span class="p">(</span><span class="nv">contentType</span><span class="p">:</span> <span class="s">"audio/ogg;codecs=opus"</span><span class="p">)</span>

<span class="c1">// uncompressed microphone audio uses a 16-bit mono PCM format at 16 kHz</span>
<span class="k">let</span> <span class="nv">settings</span> <span class="o">=</span> <span class="kt">RecognitionSettings</span><span class="p">(</span><span class="nv">contentType</span><span class="p">:</span> <span class="s">"audio/l16;rate=16000;channels=1"</span><span class="p">)</span>
</code></pre>
<h3 id='recognition-results-accumulator' class='heading'>Recognition Results Accumulator</h3>

<p>The Speech to Text service may not always return the entire transcription in a single response. Instead, the transcription may be streamed over multiple responses, each with a chunk of the overall results. This is especially common for long audio files, since the entire transcription may contain a significant amount of text.</p>

<p>To help combine multiple responses, the Swift SDK provides a <code><a href="Structs/SpeechRecognitionResultsAccumulator.html">SpeechRecognitionResultsAccumulator</a></code> object. The accumulator tracks results as they are added and maintains several useful instance variables:
    - <code>results</code>: A list of all accumulated recognition results.
    - <code>speakerLabels</code>: A list of all accumulated speaker labels.
    - <code>bestTranscript</code>: A concatenation of transcripts with the greatest confidence.</p>

<p>To use the accumulator, initialize an instance of the object then add results as you receive them:</p>
<pre class="highlight swift"><code><span class="k">var</span> <span class="nv">accumulator</span> <span class="o">=</span> <span class="kt">SpeechRecognitionResultsAccumulator</span><span class="p">()</span>
<span class="n">accumulator</span><span class="o">.</span><span class="nf">add</span><span class="p">(</span><span class="nv">results</span><span class="p">:</span> <span class="n">results</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">bestTranscript</span><span class="p">)</span>
</code></pre>
<h3 id='transcribe-recorded-audio' class='heading'>Transcribe Recorded Audio</h3>

<p>The following example demonstrates how to use the Speech to Text service to transcribe a WAV audio file.</p>
<pre class="highlight swift"><code><span class="kd">import</span> <span class="kt">SpeechToTextV1</span>

<span class="k">let</span> <span class="nv">apiKey</span> <span class="o">=</span> <span class="s">"your-api-key"</span>
<span class="k">let</span> <span class="nv">speechToText</span> <span class="o">=</span> <span class="kt">SpeechToText</span><span class="p">(</span><span class="nv">apiKey</span><span class="p">:</span> <span class="n">apiKey</span><span class="p">)</span>

<span class="k">var</span> <span class="nv">accumulator</span> <span class="o">=</span> <span class="kt">SpeechRecognitionResultsAccumulator</span><span class="p">()</span>

<span class="k">let</span> <span class="nv">audioFile</span> <span class="o">=</span> <span class="kt">Bundle</span><span class="o">.</span><span class="n">main</span><span class="o">.</span><span class="nf">url</span><span class="p">(</span><span class="nv">forResource</span><span class="p">:</span> <span class="s">"filename"</span><span class="p">,</span> <span class="nv">withExtension</span><span class="p">:</span> <span class="s">"wav"</span><span class="p">)</span><span class="o">!</span>
<span class="k">let</span> <span class="nv">settings</span> <span class="o">=</span> <span class="kt">RecognitionSettings</span><span class="p">(</span><span class="nv">contentType</span><span class="p">:</span> <span class="s">"audio/wav"</span><span class="p">)</span>
<span class="n">settings</span><span class="o">.</span><span class="n">interimResults</span> <span class="o">=</span> <span class="kc">true</span>
<span class="n">speechToText</span><span class="o">.</span><span class="nf">recognize</span><span class="p">(</span><span class="nv">audio</span><span class="p">:</span> <span class="n">audioFile</span><span class="p">,</span> <span class="nv">settings</span><span class="p">:</span> <span class="n">settings</span><span class="p">)</span> <span class="p">{</span> <span class="n">response</span><span class="p">,</span> <span class="n">error</span> <span class="k">in</span>
    <span class="k">if</span> <span class="k">let</span> <span class="nv">error</span> <span class="o">=</span> <span class="n">error</span> <span class="p">{</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">guard</span> <span class="k">let</span> <span class="nv">results</span> <span class="o">=</span> <span class="n">response</span><span class="p">?</span><span class="o">.</span><span class="n">result</span> <span class="k">else</span> <span class="p">{</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">"Failed to recognize the audio"</span><span class="p">)</span>
        <span class="k">return</span>
    <span class="p">}</span>
    <span class="n">accumulator</span><span class="o">.</span><span class="nf">add</span><span class="p">(</span><span class="nv">results</span><span class="p">:</span> <span class="n">results</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">bestTranscript</span><span class="p">)</span>
<span class="p">}</span>
</code></pre>
<h3 id='transcribe-microphone-audio' class='heading'>Transcribe Microphone Audio</h3>

<p>Audio can be streamed from the microphone to the Speech to Text service for real-time transcriptions. The following example demonstrates how to use the Speech to Text service to transcribe microphone audio:</p>
<pre class="highlight swift"><code><span class="kd">import</span> <span class="kt">SpeechToTextV1</span>

<span class="k">let</span> <span class="nv">apiKey</span> <span class="o">=</span> <span class="s">"your-api-key"</span>
<span class="k">let</span> <span class="nv">speechToText</span> <span class="o">=</span> <span class="kt">SpeechToText</span><span class="p">(</span><span class="nv">apiKey</span><span class="p">:</span> <span class="n">apiKey</span><span class="p">)</span>

<span class="k">var</span> <span class="nv">accumulator</span> <span class="o">=</span> <span class="kt">SpeechRecognitionResultsAccumulator</span><span class="p">()</span>

<span class="kd">func</span> <span class="nf">startStreaming</span><span class="p">()</span> <span class="p">{</span>
    <span class="k">var</span> <span class="nv">settings</span> <span class="o">=</span> <span class="kt">RecognitionSettings</span><span class="p">(</span><span class="nv">contentType</span><span class="p">:</span> <span class="s">"audio/ogg;codecs=opus"</span><span class="p">)</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">interimResults</span> <span class="o">=</span> <span class="kc">true</span>
    <span class="n">speechToText</span><span class="o">.</span><span class="nf">recognizeMicrophone</span><span class="p">(</span><span class="nv">settings</span><span class="p">:</span> <span class="n">settings</span><span class="p">)</span> <span class="p">{</span> <span class="n">response</span><span class="p">,</span> <span class="n">error</span> <span class="k">in</span>
        <span class="k">if</span> <span class="k">let</span> <span class="nv">error</span> <span class="o">=</span> <span class="n">error</span> <span class="p">{</span>
            <span class="nf">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
        <span class="p">}</span>
        <span class="k">guard</span> <span class="k">let</span> <span class="nv">results</span> <span class="o">=</span> <span class="n">response</span><span class="p">?</span><span class="o">.</span><span class="n">result</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nf">print</span><span class="p">(</span><span class="s">"Failed to recognize the audio"</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="p">}</span>
        <span class="n">accumulator</span><span class="o">.</span><span class="nf">add</span><span class="p">(</span><span class="nv">results</span><span class="p">:</span> <span class="n">results</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">bestTranscript</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nf">stopStreaming</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">speechToText</span><span class="o">.</span><span class="nf">stopRecognizeMicrophone</span><span class="p">()</span>
<span class="p">}</span>
</code></pre>
<h3 id='session-management-and-advanced-features' class='heading'>Session Management and Advanced Features</h3>

<p>Advanced users may want more customizability than provided by the <code><a href="Classes/SpeechToText.html">SpeechToText</a></code> class. The <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code> class exposes more control over the WebSockets connection and also includes several advanced features for accessing the microphone. The <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code> class also allows users more control over the AVAudioSession shared instance. Before using <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code>, it&rsquo;s helpful to be familiar with the <a href="https://console.bluemix.net/docs/services/speech-to-text/websockets.html">Speech to Text WebSocket interface</a>.</p>

<p>The following steps describe how to execute a recognition request with <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code>:</p>

<ol>
<li>Connect: Invoke <code>connect()</code> to connect to the service.</li>
<li>Start Recognition Request: Invoke <code>startRequest(settings:)</code> to start a recognition request.</li>
<li>Send Audio: Invoke <code>recognize(audio:)</code> or <code>startMicrophone(compress:)</code>/<code>stopMicrophone()</code> to send audio to the service.</li>
<li>Stop Recognition Request: Invoke <code>stopRequest()</code> to end the recognition request. If the recognition request is already stopped, then sending a stop message will have no effect.</li>
<li>Disconnect: Invoke <code>disconnect()</code> to wait for any remaining results to be received and then disconnect from the service.</li>
</ol>

<p>All text and data messages sent by <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code> are queued, with the exception of <code>connect()</code> which immediately connects to the server. The queue ensures that the messages are sent in-order and also buffers messages while waiting for a connection to be established. This behavior is generally transparent.</p>

<p>A <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code> also provides several (optional) callbacks. The callbacks can be used to learn about the state of the session or access microphone data.</p>

<ul>
<li><code>onConnect</code>: Invoked when the session connects to the Speech to Text service.</li>
<li><code>onMicrophoneData</code>: Invoked with microphone audio when a recording audio queue buffer has been filled. If microphone audio is being compressed, then the audio data is in OggOpus format. If uncompressed, then the audio data is in 16-bit PCM format at 16 kHz.</li>
<li><code>onPowerData</code>: Invoked every 0.025s when recording with the average dB power of the microphone.</li>
<li><code>onResults</code>: Invoked when transcription results are received for a recognition request.</li>
<li><code>onError</code>: Invoked when an error or warning occurs.</li>
<li><code>onDisconnect</code>: Invoked when the session disconnects from the Speech to Text service.</li>
</ul>

<p>Note that the <code>AVAudioSession.sharedInstance()</code> must be configured to allow microphone access when using <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code>. This allows users to set a particular configuration for the <code>AVAudioSession</code>. An example configuration is shown in the code below.</p>

<p>The following example demonstrates how to use <code><a href="Classes/SpeechToTextSession.html">SpeechToTextSession</a></code> to transcribe microphone audio:</p>
<pre class="highlight swift"><code><span class="kd">import</span> <span class="kt">SpeechToTextV1</span>

<span class="k">let</span> <span class="nv">apiKey</span> <span class="o">=</span> <span class="s">"your-api-key"</span>
<span class="k">let</span> <span class="nv">speechToTextSession</span> <span class="o">=</span> <span class="kt">SpeechToTextSession</span><span class="p">(</span><span class="nv">apiKey</span><span class="p">:</span> <span class="n">apiKey</span><span class="p">)</span>

<span class="k">var</span> <span class="nv">accumulator</span> <span class="o">=</span> <span class="kt">SpeechRecognitionResultsAccumulator</span><span class="p">()</span>

<span class="k">do</span> <span class="p">{</span>
    <span class="k">let</span> <span class="nv">session</span> <span class="o">=</span> <span class="kt">AVAudioSession</span><span class="o">.</span><span class="nf">sharedInstance</span><span class="p">()</span>
    <span class="k">try</span> <span class="n">session</span><span class="o">.</span><span class="nf">setCategory</span><span class="p">(</span><span class="kt">AVAudioSession</span><span class="o">.</span><span class="kt">Category</span><span class="o">.</span><span class="n">playAndRecord</span><span class="p">,</span> <span class="nv">mode</span><span class="p">:</span> <span class="o">.</span><span class="k">default</span><span class="p">,</span> <span class="nv">options</span><span class="p">:</span> <span class="p">[</span><span class="o">.</span><span class="n">defaultToSpeaker</span><span class="p">,</span> <span class="o">.</span><span class="n">mixWithOthers</span><span class="p">])</span>
    <span class="k">try</span> <span class="n">session</span><span class="o">.</span><span class="nf">setActive</span><span class="p">(</span><span class="kc">true</span><span class="p">)</span>
<span class="p">}</span> <span class="k">catch</span> <span class="p">{</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nf">startStreaming</span><span class="p">()</span> <span class="p">{</span>
    <span class="c1">// define callbacks</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="n">onConnect</span> <span class="o">=</span> <span class="p">{</span> <span class="nf">print</span><span class="p">(</span><span class="s">"connected"</span><span class="p">)</span> <span class="p">}</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="n">onDisconnect</span> <span class="o">=</span> <span class="p">{</span> <span class="nf">print</span><span class="p">(</span><span class="s">"disconnected"</span><span class="p">)</span> <span class="p">}</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="n">onError</span> <span class="o">=</span> <span class="p">{</span> <span class="n">error</span> <span class="k">in</span> <span class="nf">print</span><span class="p">(</span><span class="n">error</span><span class="p">)</span> <span class="p">}</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="n">onPowerData</span> <span class="o">=</span> <span class="p">{</span> <span class="n">decibels</span> <span class="k">in</span> <span class="nf">print</span><span class="p">(</span><span class="n">decibels</span><span class="p">)</span> <span class="p">}</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="n">onMicrophoneData</span> <span class="o">=</span> <span class="p">{</span> <span class="n">data</span> <span class="k">in</span> <span class="nf">print</span><span class="p">(</span><span class="s">"received data"</span><span class="p">)</span> <span class="p">}</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="n">onResults</span> <span class="o">=</span> <span class="p">{</span> <span class="n">results</span> <span class="k">in</span>
        <span class="n">accumulator</span><span class="o">.</span><span class="nf">add</span><span class="p">(</span><span class="nv">results</span><span class="p">:</span> <span class="n">results</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">accumulator</span><span class="o">.</span><span class="n">bestTranscript</span><span class="p">)</span>
    <span class="p">}</span>

    <span class="c1">// define recognition request settings</span>
    <span class="k">var</span> <span class="nv">settings</span> <span class="o">=</span> <span class="kt">RecognitionSettings</span><span class="p">(</span><span class="nv">contentType</span><span class="p">:</span> <span class="s">"audio/ogg;codecs=opus"</span><span class="p">)</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">interimResults</span> <span class="o">=</span> <span class="kc">true</span>

    <span class="c1">// start streaming microphone audio for transcription</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="nf">connect</span><span class="p">()</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="nf">startRequest</span><span class="p">(</span><span class="nv">settings</span><span class="p">:</span> <span class="n">settings</span><span class="p">)</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="nf">startMicrophone</span><span class="p">()</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nf">stopStreaming</span><span class="p">()</span> <span class="p">{</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="nf">stopMicrophone</span><span class="p">()</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="nf">stopRequest</span><span class="p">()</span>
    <span class="n">speechToTextSession</span><span class="o">.</span><span class="nf">disconnect</span><span class="p">()</span>
<span class="p">}</span>
</code></pre>
<h3 id='additional-information' class='heading'>Additional Information</h3>

<p>There are a number of ways that Speech to Text can be customized to suit your particular application. For example, you can define custom words or upload audio to train an acoustic model. The following links provide more information about the IBM Speech to Text service:</p>

<ul>
<li><a href="https://www.ibm.com/watson/services/speech-to-text/">IBM Watson Speech to Text - Service Page</a></li>
<li><a href="https://console.bluemix.net/docs/services/speech-to-text/index.html">IBM Watson Speech to Text - Documentation</a></li>
<li><a href="https://speech-to-text-demo.ng.bluemix.net/">IBM Watson Speech to Text - Demo</a></li>
</ul>

          </section>
        </section>
        <section id="footer">
          <p>&copy; 2019 <a class="link" href="https://www.ibm.com/watson/ai-assistant/" target="_blank" rel="external">Anthony Oliveri and Mike Kistler</a>. All rights reserved. (Last updated: 2019-01-18)</p>
          <p>Generated by <a class="link" href="https://github.com/realm/jazzy" target="_blank" rel="external">jazzy ♪♫ v0.9.4</a>, a <a class="link" href="https://realm.io" target="_blank" rel="external">Realm</a> project.</p>
        </section>
      </article>
    </div>
  </body>
</div>
</html>
